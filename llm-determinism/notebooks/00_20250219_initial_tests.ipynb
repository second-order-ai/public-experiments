{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial tests\n",
    "\n",
    "Testing the sample code from perplexity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependencies:\n",
    "\n",
    "* Ended up with `uv pip install torch==2.1.0` as `uv add torch` failed with: `error: Distribution `torch==2.6.0 @ registry+https://pypi.org/simple` can't be installed because it doesn't have a source distribution or wheel for the current platform`\n",
    "* Got a numpy vs numba warning, indicating I should downgrade numpy. I did not oblige."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model and tokenizer, this will take a while (took around 2.3min with 1GB/s fiber)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:13<00:00, 66.78s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically, we are going to tokenize our inputs, then plug that into the model, get tokens back as output, and convert that back into words (I think).\n",
    "\n",
    "Step one, tokenizing the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 464, 3139,  286, 4881,  318]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_text = \"The capital of France is\"\n",
    "input_ids = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    ").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such pretty numbers.\n",
    "\n",
    "Next we set the parameters that affects the stochastic nature of LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should produce deterministic results, should..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112319630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get back the tokens and their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6342,    13,   198, 26410,    25,   383, 23227,    82,   287,   262,\n",
      "         6827,   389,   366, 28572,  1600,   366, 27544,     1,   290,   366])\n",
      "tensor([[[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[   -inf, 34.7760,    -inf,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]])\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = outputs.sequences[0, input_ids.shape[-1]:]\n",
    "scores = torch.stack(outputs.scores)\n",
    "print(generated_tokens)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some `-inf`, and some non inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.log_softmax(scores, dim=-1)\n",
    "token_indices = generated_tokens.reshape(1, -1, 1).expand(1, -1, 1)\n",
    "token_indices = token_indices.transpose(0, 1)\n",
    "token_log_probs = log_probs.gather(-1, token_indices).squeeze(-1)\n",
    "token_probs = torch.exp(token_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities of the final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      " ```\n",
      " Paris.\n",
      "Output: The nouns in the sentence are \"France\", \"capital\" and \"\n",
      "```\n",
      "\n",
      "Per-token probabilities:\n",
      "` Paris`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "`.`\t\t: prob=0.6372 \t\t (log_prob=-0.4507)\n",
      "`\n",
      "`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "`Output`\t\t: prob=0.0987 \t\t (log_prob=-2.3158)\n",
      "`:`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` The`\t\t: prob=0.6562 \t\t (log_prob=-0.4213)\n",
      "` noun`\t\t: prob=0.0128 \t\t (log_prob=-4.3564)\n",
      "`s`\t\t: prob=0.5722 \t\t (log_prob=-0.5583)\n",
      "` in`\t\t: prob=0.4682 \t\t (log_prob=-0.7589)\n",
      "` the`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` sentence`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` are`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` \"`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "`France`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "`\",`\t\t: prob=0.4187 \t\t (log_prob=-0.8705)\n",
      "` \"`\t\t: prob=0.7697 \t\t (log_prob=-0.2617)\n",
      "`capital`\t\t: prob=0.5945 \t\t (log_prob=-0.5201)\n",
      "`\"`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` and`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n",
      "` \"`\t\t: prob=1.0000 \t\t (log_prob=0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Generated text:\\n\\n ```\\n{tokenizer.decode(generated_tokens)}\\n```\")\n",
    "print(\"\\nPer-token probabilities:\")\n",
    "for token, log_prob, prob in zip(generated_tokens, token_log_probs, token_probs):\n",
    "    token_text = tokenizer.decode([token])\n",
    "    print(f\"`{token_text}`\\t\\t: prob={prob.item():.4f} \\t\\t (log_prob={log_prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bit that the model produces: `Output: The nouns in the sentence are \"France\", \"capital\" and \"` is strange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the top 5 at each step, which is much more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      " ```\n",
      " Paris.\n",
      "Output: The nouns in the sentence are \"France\", \"capital\" and \"\n",
      "```\n",
      "\n",
      "Top 5 probable tokens at each step:\n",
      "\n",
      "Step 0: Actually chosen token: ' Paris'\n",
      "Top 5 candidates:\n",
      "\t ` Paris`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 1: Actually chosen token: '.'\n",
      "Top 5 candidates:\n",
      "\t `.`: \t 0.6372\n",
      "\t `.\"`: \t 0.3186\n",
      "\t `,`: \t 0.0442\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\n",
      "Step 2: Actually chosen token: '\n",
      "'\n",
      "Top 5 candidates:\n",
      "\t `\n",
      "`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 3: Actually chosen token: 'Output'\n",
      "Top 5 candidates:\n",
      "\t `<|endoftext|>`: \t 0.5940\n",
      "\t `    `: \t 0.2170\n",
      "\t `Output`: \t 0.0987\n",
      "\t `Answer`: \t 0.0516\n",
      "\t `##`: \t 0.0386\n",
      "\n",
      "Step 4: Actually chosen token: ':'\n",
      "Top 5 candidates:\n",
      "\t `:`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 5: Actually chosen token: ' The'\n",
      "Top 5 candidates:\n",
      "\t ` The`: \t 0.6562\n",
      "\t ` Fact`: \t 0.1660\n",
      "\t ` This`: \t 0.0538\n",
      "\t ` True`: \t 0.0362\n",
      "\t ` What`: \t 0.0335\n",
      "\n",
      "Step 6: Actually chosen token: ' noun'\n",
      "Top 5 candidates:\n",
      "\t ` capital`: \t 0.5242\n",
      "\t ` sentence`: \t 0.1988\n",
      "\t ` correct`: \t 0.0738\n",
      "\t ` main`: \t 0.0524\n",
      "\t ` statement`: \t 0.0441\n",
      "\n",
      "Step 7: Actually chosen token: 's'\n",
      "Top 5 candidates:\n",
      "\t `s`: \t 0.5722\n",
      "\t ` in`: \t 0.4278\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\n",
      "Step 8: Actually chosen token: ' in'\n",
      "Top 5 candidates:\n",
      "\t ` present`: \t 0.5318\n",
      "\t ` in`: \t 0.4682\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\n",
      "Step 9: Actually chosen token: ' the'\n",
      "Top 5 candidates:\n",
      "\t ` the`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 10: Actually chosen token: ' sentence'\n",
      "Top 5 candidates:\n",
      "\t ` sentence`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 11: Actually chosen token: ' are'\n",
      "Top 5 candidates:\n",
      "\t ` are`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 12: Actually chosen token: ' \"'\n",
      "Top 5 candidates:\n",
      "\t ` \"`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 13: Actually chosen token: 'France'\n",
      "Top 5 candidates:\n",
      "\t `France`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 14: Actually chosen token: '\",'\n",
      "Top 5 candidates:\n",
      "\t `\"`: \t 0.5813\n",
      "\t `\",`: \t 0.4187\n",
      "\t `#`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\n",
      "Step 15: Actually chosen token: ' \"'\n",
      "Top 5 candidates:\n",
      "\t ` \"`: \t 0.7697\n",
      "\t ` and`: \t 0.2303\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\n",
      "Step 16: Actually chosen token: 'capital'\n",
      "Top 5 candidates:\n",
      "\t `capital`: \t 0.5945\n",
      "\t `Paris`: \t 0.4055\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\n",
      "Step 17: Actually chosen token: '\"'\n",
      "Top 5 candidates:\n",
      "\t `\"`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `$`: \t 0.0000\n",
      "\n",
      "Step 18: Actually chosen token: ' and'\n",
      "Top 5 candidates:\n",
      "\t ` and`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n",
      "\n",
      "Step 19: Actually chosen token: ' \"'\n",
      "Top 5 candidates:\n",
      "\t ` \"`: \t 1.0000\n",
      "\t `#`: \t 0.0000\n",
      "\t `%`: \t 0.0000\n",
      "\t `!`: \t 0.0000\n",
      "\t `\"`: \t 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get scores from output\n",
    "scores = torch.stack(outputs.scores)  # Shape: [num_tokens, 1, vocab_size]\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "# Get top 5 probabilities and indices for each step\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "\n",
    "# Print results for each generation step\n",
    "print(f\"Generated text:\\n\\n ```\\n{tokenizer.decode(generated_tokens)}\\n```\")\n",
    "print(\"\\nTop 5 probable tokens at each step:\")\n",
    "for step, (token, step_probs, step_indices) in enumerate(zip(generated_tokens, top_probs, top_indices)):\n",
    "    actual_token = tokenizer.decode([token])\n",
    "    print(f\"\\nStep {step}: Actually chosen token: '{actual_token}'\")\n",
    "    print(\"Top 5 candidates:\")\n",
    "    for prob, idx in zip(step_probs[0], step_indices[0]):\n",
    "        token_text = tokenizer.decode([idx])\n",
    "        print(f\"\\t `{token_text}`: \\t {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting Step 3 is fun now:\n",
    "\n",
    "```\n",
    "Step 3: Actually chosen token: 'Output'\n",
    "Top 5 candidates:\n",
    "\t `<|endoftext|>`: \t 0.5940\n",
    "\t `    `: \t 0.2170\n",
    "\t `Output`: \t 0.0987\n",
    "\t `Answer`: \t 0.0516\n",
    "\t `##`: \t 0.0386\n",
    "```\n",
    "\n",
    "The highest probability ($p=0.5940$) token was `<|endoftext|>`, which would have stopped the token generation. The result would then just have been `Paris`. Which is correct. Due to the stochastic nature of LLMs, it chose the lower probability token ($p=0.0987$) `Output`. This is just a fancy way of saying that it does weighting sampling based on the calculated probabilities, which in turn are calculated by the Neural-Network. So the higher the probability of a token, the higher the chances that it will get selected. At Step 3, if we were to repeat this process 1000 times, we would expect it to stop (i.e. `<|endoftext|>`) about 600 times, introduce a tab `    ` about 210 times, produce `Output` about 100 times, and so forth. The actual result that we saw is the 10\\% probability materializing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is just a test example to get going, and it's already quite interesting. For more formal testing, we can wrap the above into functions, repeat the experiment, and capture and analyze the results. Next, we can adjust the input parameters to make it deterministic. Then we can tweak the inputs slightly, to see how it influences the results.\n",
    "\n",
    "We can also run some stretch experiments, such as using more difficult prompts, in the sense that there is less training data, and use different models.\n",
    "\n",
    "Ultimately, this should allow us to get a better sense of the non-deterministic nature of LLMs and how to control it. LLMs have sharp edges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
